{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bcdfc32f-ac43-480b-a730-cdb2f44af7d0",
      "metadata": {},
      "source": [
        "# Fast exponential and softmax functions\n",
        "\n",
        "Based on the bit manupulation of the IEEE 754 floating point numbers.\n",
        "See N. N. Schraudolph, “A Fast, Compact Approximation of the Exponential Function,” Neural Computation, vol. 11, no. 4, pp. 853–862, May 1999, doi: 10.1162/089976699300016467.\n",
        "\n",
        "A 32-bit floating point number is given by:\n",
        "$$ x = (-1)^s (1+m)2^{e-127} $$\n",
        "where $s$ - sign bit; $m$ - 23-bit mantissa; $e$ - 8-bit exponent;\n",
        "\n",
        "For bit layout see [this diagram](https://upload.wikimedia.org/wikipedia/commons/d/d2/Float_example.svg).\n",
        "\n",
        "Writing some integer value $i$ in bit field of the exponent and read back as a float number will give value of$2^{i-127}$. \n",
        "\n",
        "So dividing $x$ by dividing by $\\ln(2)$, shifting the value $x/\\ln(2)+127$ by 23 bit, and reading back the bits of resulting integer number as a flot will give value of the $e^x$. Correction factor $C$ allows adjusting the approximation for some parameters.\n",
        "\n",
        "$$f(x) = 2^{23} \\left(\\frac{x}{\\ln(2)} + 127 - C\\right)$$\n",
        "In some example following approximation is used with specific values in code $C = 0.057985$:\n",
        "```C\n",
        "uint32_t value = (1<<23)*(1.4426950409*x+126.94201519f);\n",
        "```\n",
        "Which is then bitcast to floating point number via union type punning.\n",
        "\n",
        "\n",
        "## Softmax\n",
        "\n",
        "Softmax is a function over a vector $\\mathbf{X}$:\n",
        "\n",
        "$$ \\sigma(\\mathbf{X})_i = \\frac{e^{x_i}}{\\sum\\limits_{j=0}^{N}e^{x_j}} $$\n",
        "\n",
        "\n",
        "When calculating softmax the maximum is substracted from initial data for numerical stability - to avoid overflows.\n",
        "\n",
        "## Further reading\n",
        "\n",
        "F. Perini and R. D. Reitz, “Fast approximations of exponential and logarithm functions combined with efficient storage/retrieval for combustion kinetics calculations,” Combustion and Flame, vol. 194, pp. 37–51, 2018, doi: 10.1016/j.combustflame.2018.04.013.\n",
        "\n",
        "J.-M. Muller, “Elementary Functions and Approximate Computing,” Proc. IEEE, vol. 108, no. 12, pp. 2136–2149, 2020, doi: 10.1109/JPROC.2020.2991885.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50603670-4723-4aef-a4b6-50ff144295ce",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from functools import partial\n",
        "\n",
        "SIZE = 10000\n",
        "\n",
        "C = 127 - 126.94201519\n",
        "LOG2_e = np.float32(1.4426950409)\n",
        "\n",
        "print(\"C = {:f}\".format(C))\n",
        "print(\"1/log2(e)\", 1 / np.log2(np.e))\n",
        "\n",
        "print(\"A: {:0.1f}\".format(LOG2_e * np.float32(1 << 23)))\n",
        "print(\"C: {:0.1f}\".format(np.float32(126.94201519) * np.float32(1 << 23)))\n",
        "\n",
        "print(\"f64 A: {:0.1f}/ln(2) = {:0.1f}\".format(np.float64(1 << 20), LOG2_e * np.float64(1 << 20)))\n",
        "print(\"f64 C: {:0.1f}-{:0.1f}={:0.1f}\".format(1023 * np.float32(1 << 20), 60801, 1023 * np.float32(1 << 20) - 60801))\n",
        "\n",
        "\n",
        "def fast_exp(x, c=C):\n",
        "    A = LOG2_e * np.ones((SIZE), np.float32)\n",
        "    K = np.float32(1 << 23) * np.ones((SIZE), np.float32)\n",
        "    y = np.multiply(K, (A * x + 127 - c), dtype=\"f\")\n",
        "    z = np.int32(y)\n",
        "    v = z.view(np.float32)\n",
        "    return v\n",
        "\n",
        "def delta_yf_3(yf):\n",
        "    s0 = np.float32(0)\n",
        "    s1 = np.float32(0.30758037765820823 )\n",
        "    s2 = np.float32(-0.23141283591588344 )\n",
        "    s3 = np.float32(-7.6167541742324804e-2)\n",
        "    return s1 * yf + s2 * yf * yf + s3 * yf * yf * yf\n",
        "\n",
        "def delta_yf_5(yf):\n",
        "    s0 = np.float32(0)\n",
        "    s1 = np.float32( 3.06852819440055e-1)\n",
        "    s2 = np.float32(-2.40226506959101e-1)\n",
        "    s3 = np.float32(-5.57129652016652e-2)\n",
        "    s4 = np.float32(-9.01146535969578e-3)\n",
        "    s5 = np.float32(-1.90188191959304e-3)\n",
        "    p1 = yf\n",
        "    p2 = p1 * yf\n",
        "    p3 = p2 * yf\n",
        "    p4 = p3 * yf\n",
        "    p5 = p4 * yf\n",
        "    return s1 * p1 + s2 * p2 + s3 * p3 + s4 * p4 + s5 * p5\n",
        "\n",
        "def perini_taylor(yf):\n",
        "    return 1 + yf - 2**yf\n",
        "\n",
        "def fast_exp_delta(x, corr):\n",
        "    y = np.float32(x * LOG2_e)\n",
        "    yf = y - np.int32(y) + 1 # y < 0\n",
        "    yf[y>0] = yf[y>0] - 1\n",
        "    y2 = np.int32((1 << 23) * (y - corr(yf) + 127))\n",
        "    return y2.view(np.float32)\n",
        "\n",
        "def fast_log(x):\n",
        "    sigma = 0.0430357\n",
        "    v = x.view(np.int32)\n",
        "    y = 0.6931471805599453 * np.float32(v / (2**23) - (127 - sigma))\n",
        "    return np.float32(y)\n",
        "\n",
        "\n",
        "def fast_exp_newtons(x, exp_func, log_func, q=1):\n",
        "    y = exp_func(x)\n",
        "    for _ in range(q):\n",
        "        y = y - y * log_func(y) + y * x\n",
        "    return y\n",
        "\n",
        "\n",
        "def bounds():\n",
        "    print(\"Upper bound: {:f}\".format(fast_exp([0], -1)[0]))\n",
        "    print(\n",
        "        \"Lower bound: {:f}\".format(2**23 * ((1 - (np.log(np.log(2)) + 1)) / np.log(2)))\n",
        "    )\n",
        "    gamma = np.log(np.log(2) + 2 / np.e) - np.log(2) - np.log(np.log(2))\n",
        "    c = gamma * 2**23 / np.log(2)\n",
        "    print(\"Lowest maximum relative error: gamma = {:f}\".format(gamma))\n",
        "    print(\"Lowest maximum relative error: c = {:f} {:f}\".format(c, c / 2**23))\n",
        "    c = np.log(3 / (8 * np.log(2)) + 0.5) / np.log(2)\n",
        "    print(\"Lowest RMS error: c = {:f} {:f}\".format(2**23 * c, c))\n",
        "    gamma = 0.045111411\n",
        "    c = gamma / np.log(2)\n",
        "    print(\"Lowest mean error: c = {:f} {:f}\".format(2**23 * c, c))\n",
        "\n",
        "\n",
        "def softmax(x, func):\n",
        "    exps = func(x)\n",
        "    sums = np.sum(exps)\n",
        "    return func(x) / sums\n",
        "\n",
        "\n",
        "def get_err(x, tr, app):\n",
        "    return 100 * (tr(x) - app(x)) / tr(x)\n",
        "\n",
        "\n",
        "def plot(x, funcs, labels):\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
        "    ax1.grid(True)\n",
        "    ax2.grid(True)\n",
        "    i = 0\n",
        "    for func in funcs:\n",
        "        ax1.plot(x, func(x))\n",
        "        err = get_err(x, funcs[0], func)\n",
        "        ax2.plot(x, err)\n",
        "        print(\n",
        "            \"{:25s} | max err = {:f} | rms err = {:f} \".format(\n",
        "                labels[i], max(np.abs(err)), np.sqrt(np.mean(err**2))\n",
        "            )\n",
        "        )\n",
        "        i = i + 1\n",
        "    ax1.legend(labels)\n",
        "    ax2.legend(labels)\n",
        "    ax1.set_xlabel(\"x\")\n",
        "    ax1.set_ylabel(\"f(x)\")\n",
        "    ax2.set_xlabel(\"x\")\n",
        "    ax2.set_ylabel(\"error, %\")\n",
        "\n",
        "\n",
        "bounds()\n",
        "\n",
        "x = np.linspace(-2, 2, SIZE, dtype=np.float32)\n",
        "\n",
        "fast_exp_3 = partial(fast_exp_delta, corr=delta_yf_3)\n",
        "fast_exp_5 = partial(fast_exp_delta, corr=delta_yf_5)\n",
        "\n",
        "plot(\n",
        "    x,\n",
        "    [\n",
        "        np.exp,\n",
        "        fast_exp,\n",
        "        fast_exp_3,\n",
        "        fast_exp_5,\n",
        "    ],\n",
        "    [\n",
        "        \"True\",\n",
        "        \"fast_exp\",\n",
        "        \"fast_exp_3\",\n",
        "        \"fast_exp_5\",\n",
        "    ],\n",
        ")\n",
        "print(\"softmax\")\n",
        "plot(\n",
        "    x,\n",
        "    [\n",
        "        partial(softmax, func=np.exp),\n",
        "        partial(softmax, func=fast_exp),\n",
        "        partial(softmax, func=fast_exp_3),\n",
        "        partial(softmax, func=fast_exp_5),\n",
        "    ],\n",
        "    [\n",
        "        \"True\",\n",
        "        \"fast_exp\",\n",
        "        \"fast_exp_3\",\n",
        "        \"fast_exp_5\",\n",
        "    ],\n",
        ")\n",
        "# x2 = np.linspace(0.5, 10.0, SIZE, dtype=np.float32)\n",
        "# plot(x2, [np.log, fast_log], [\"True\", \"Fast log\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ddf7074",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "SIZE = 133\n",
        "x = np.float32(np.random.rand(SIZE))\n",
        "def softmax(x, func):\n",
        "    exps = func(x)\n",
        "    sums = np.sum(exps)\n",
        "    return func(x) / sums\n",
        "\n",
        "with open(\"../bin/data\", \"wb\") as file:\n",
        "    file.write(x)\n",
        "\n",
        "with open(\"../bin/golden\", \"wb\") as file:\n",
        "    file.write(softmax(x, np.exp))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
